{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "reported-rabbit",
   "metadata": {},
   "source": [
    "## Comparison Approach\n",
    "This notebook loads each of the individual, trained models from the best runs of BertForSequenceClassification. It will show the model.summary() and diagram, then will run a performance test by inferring results for the texts in the ClaimBuster dataset's crowdsourced.csv file. The file contains 22501 sentences. We will use sentences per second as the performance metric, and the number of parameters in the model as a secondary metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stupid-district",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Usual Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as backend\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.config.experimental import list_physical_devices, set_visible_devices\n",
    "\n",
    "import string\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import matplotlib as plt\n",
    "\n",
    "import datetime\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../python')\n",
    "import debug\n",
    "from jbyrne_utils import tokenize_sentences\n",
    "\n",
    "\n",
    "# to fix the CUDA issues for CUDA 11.2 to allow use of the GPU\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-brooks",
   "metadata": {},
   "source": [
    "### Load the crowdsourced test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "amber-lloyd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and parse the crowdsourced.csv file\n",
    "\n",
    "cs = pd.read_csv(\"../data/crowdsourced.csv\", delimiter=',', quotechar = '\"', index_col='Sentence_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-badge",
   "metadata": {},
   "source": [
    "Unlike the curated json dataset we used for training, the \"verdict\" column takes three values:\n",
    "\n",
    "| Verdict | Description |\n",
    "| :---: | :--- |\n",
    "| +1 | Checkable Fact Statements, e.g. \"Inflation is down 2%\" |\n",
    "| 0 | Uncheckable Fact Statements, e.g. \"Jack likes fish\" |\n",
    "| -1 | Non Fact Statements, e.g. \"Drink the water\" |\n",
    "\n",
    "For the purposes of this paper, we are only interested in checkable fact statements, so we set any -1 verdicts to equal zero before tokenizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "manufactured-zambia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14685"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cs.loc[cs[\"Verdict\"] == -1][\"Verdict\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "scientific-solution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 14685 -1 labels.\n",
      "After:  0 -1 labels.\n"
     ]
    }
   ],
   "source": [
    "# Change -1 verdicts (non claim sentences) to be 0.\n",
    "print(f\"Before: {len(cs.loc[cs['Verdict'] == -1])} -1 labels.\")\n",
    "\n",
    "cs.loc[cs[\"Verdict\"] == -1, \"Verdict\"] = 0\n",
    "\n",
    "print(f\"After:  {len(cs.loc[cs['Verdict'] == -1])} -1 labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-amber",
   "metadata": {},
   "source": [
    "### Tokenizing the new dataset\n",
    "Provided this is run AFTER the other tests, there should be a tokenizer.pkl and embed_matrix.pkl already created from the training dataset.  We need to encode the new text using the same vocabulary and ID mapping as it will be input into a pre-trained embeddings layer in the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-scott",
   "metadata": {},
   "source": [
    "## Initialize the output dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "latter-somalia",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(columns = [\"Type\",\n",
    "                                 \"Model\",\n",
    "                                 \"Hardware\",\n",
    "                                 \"Max Length\",\n",
    "                                 \"Filters\",\n",
    "                                 \"Dense Layers\",\n",
    "                                 \"Parameter Count\",\n",
    "                                 \"Val Accuracy\",\n",
    "                                 \"Test Accuracy\",\n",
    "                                 \"Inf. Rate/s\",\n",
    "                                 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-astrology",
   "metadata": {},
   "source": [
    "### GPU vs CPU performance\n",
    "As one objective is to run claim detection at the edge, we will be doing performance testing on both GPU and CPU hardware.\n",
    "\n",
    "All work on this project has been done using the following software and hardware:\n",
    "\n",
    "Anaconda distribution of Python 3.8.2\n",
    "Tensorflow 2.4.1\n",
    "\n",
    "AMD Ryzen TR 3970X 32-Core Processor with Hyperthreading (64 threads)\n",
    "NVidia RTX2080 Super GPU\n",
    "\n",
    "First display the tensorflow IDs for the CPU and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "innocent-gospel",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_physical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-democracy",
   "metadata": {},
   "source": [
    "## Test the CNN Best models for each max_len\n",
    "We ran a series of grid searches to find the best performing models for each of the tested sentence lengths:\n",
    "\n",
    "| max_len | Description |\n",
    "| :---: | :--- |\n",
    "| 100 | Only 14 of 11056 sentences are truncated, so 99.87% of sentences are processed in full |\n",
    "| 50 | Half the length still processes 95.57% of sentences in full |\n",
    "| 21 | Equal to the rounded average length, processes 61.478% of sentences in full |\n",
    "| 17 | Equal to the median length, processes half the sentences in full |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-badge",
   "metadata": {},
   "source": [
    "The following was the output from the hyperparameter tuinng for the Bert model:\n",
    "```\n",
    "Results summary\n",
    "Results in ./tb_bert_tuner/210404-235430/BertTunerRandom\n",
    "Showing 10 best trials\n",
    "Objective(name='val_accuracy', direction='max')\n",
    "Trial summary\n",
    "Hyperparameters:\n",
    "bert_trainable: 1\n",
    "learning_rate: 5e-06\n",
    "epsilon: 5e-05\n",
    "Score: 0.9327305555343628\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "weekly-burst",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: ../best_models/bert_keras_tuner/{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-71102382c75a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mtimestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"210404-235430\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'../best_models/bert_keras_tuner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n\\n\\nCNN Model from timestamp {timestamp}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/w266/lib/python3.8/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    209\u001b[0m       \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/w266/lib/python3.8/site-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    109\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot parse file %s: %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpath_to_pbtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     raise IOError(\"SavedModel file does not exist at: %s/{%s|%s}\" %\n\u001b[0m\u001b[1;32m    112\u001b[0m                   (export_dir,\n\u001b[1;32m    113\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: ../best_models/bert_keras_tuner/{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "cycles = 5\n",
    "max_len = 100\n",
    "\n",
    "def run_perftest(tokens, labels, model, cycles):\n",
    "    print(f\"Inferring {cycles} iterations of test data\")\n",
    "    start_time = datetime.datetime.now()\n",
    "    for i in range(cycles):\n",
    "        history = model.evaluate(tokens, labels, batch_size=128, verbose=0)\n",
    "        if i % 10 == 0:\n",
    "            print(f\"\\n{i:03d}\", end=\"\")\n",
    "        else:\n",
    "            print(\".\", end=\"\")\n",
    "    print('\\n\\nCOMPLETED\\n')\n",
    "                  \n",
    "    end_time = datetime.datetime.now()\n",
    "    difference = end_time - start_time\n",
    "    return (difference.total_seconds(), history)\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "timestamp = \"210404-235430\"\n",
    "\n",
    "model = keras.models.load_model(f'../best_models/bert_keras_tuner')\n",
    "print(f\"\\n\\n\\nCNN Model from timestamp {timestamp}\")\n",
    "\n",
    "# Display summary and diagram of the model\n",
    "model.summary()\n",
    "keras.utils.plot_model(model, f'{timestamp}.png', show_shapes=True, show_dtype=True, rankdir=\"TB\")\n",
    "\n",
    "tokens, _ = tokenize_sentences(cs[\"Text\"], max_len=max_len )\n",
    "labels = cs[\"Verdict\"]\n",
    "\n",
    "# Run Performance Test on the crowdsourced test data set\n",
    "\n",
    "difference, history = run_perftest(tokens, labels, model, cycles)\n",
    "\n",
    "print(f\"Time taken for {cycles * len(labels):,} inferrences = {difference:.3f} s.\")\n",
    "print(f\"Rate is {cycles * len(labels) / difference:,.3f} inferrences per second\")\n",
    "\n",
    "# Add the results to the output\n",
    "\n",
    "parameter_count = np.sum([backend.count_params(w) for w in model.trainable_weights]) + \\\n",
    "                  np.sum([backend.count_params(w) for w in model.non_trainable_weights])\n",
    "\n",
    "\n",
    "record = pd.DataFrame( {\"Type\": \"Bert\",\n",
    "                        \"Model\": timestamp,\n",
    "                        \"Hardware\": \"GPU\",\n",
    "                        \"Max Length\": max_len,\n",
    "                        \"Filters\": \"n/a\",\n",
    "                        \"Dense Layers\": \"n/a\",\n",
    "                        \"Parameter Count\": parameter_count ,\n",
    "                        \"Val Accuracy\": 0.9327305555343628,\n",
    "                        \"Test Accuracy\": history[1],\n",
    "                        \"Inf. Rate/s\": cycles * len(labels) / difference\n",
    "                       },\n",
    "                       index = [1]) # timestamp\n",
    "output = output.append(record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-waste",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-christopher",
   "metadata": {},
   "source": [
    "__210409-235328: max_len = 50__\n",
    "![\"210409-235328\"](./210409-235328.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-marking",
   "metadata": {},
   "source": [
    "__210409-210515\tmax_len=100__\n",
    "![\"210409-210515\"](./210409-210515.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-fifth",
   "metadata": {},
   "source": [
    "__210409-225707\tmax_len=17__\n",
    "![\"210409-225707\"](./210409-225707.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-lloyd",
   "metadata": {},
   "source": [
    "__210410-004542\tmax_len=21__\n",
    "![\"210410-004542\"](./210410-004542.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-lawsuit",
   "metadata": {},
   "source": [
    "### Citations\n",
    "@inproceedings{arslan2020claimbuster,\n",
    "    title={{A Benchmark Dataset of Check-worthy Factual Claims}},\n",
    "    author={Arslan, Fatma and Hassan, Naeemul and Li, Chengkai and Tremayne, Mark },\n",
    "    booktitle={14th International AAAI Conference on Web and Social Media},\n",
    "    year={2020},\n",
    "    organization={AAAI}\n",
    "}\n",
    "\n",
    "@article{meng2020gradient,\n",
    "  title={Gradient-Based Adversarial Training on Transformer Networks for Detecting Check-Worthy Factual Claims},\n",
    "  author={Meng, Kevin and Jimenez, Damian and Arslan, Fatma and Devasier, Jacob Daniel and Obembe, Daniel and Li, Chengkai},\n",
    "  journal={arXiv preprint arXiv:2002.07725},\n",
    "  year={2020}\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
