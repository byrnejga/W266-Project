{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "conceptual-canal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow_text\n",
    "# !pip install transformers\n",
    "# !pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "round-grant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** DEBUG DETAILS TURNED ON *****************\n"
     ]
    }
   ],
   "source": [
    "## Usual Imports\n",
    "\n",
    "## Math and Arrays\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "from statistics import mean\n",
    "\n",
    "# OS and Utilities\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "## File and String Handling\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# BERT\n",
    "from transformers import *\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "\n",
    "# Tensorflow 2 core - preprocessing no longer needed as we are using BERT\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from tensorflow import keras\n",
    "\n",
    "#Import Tuining toolkit for automatic Hyperparameter search\n",
    "import kerastuner as kt\n",
    "\n",
    "# from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Add local path to .py modules and add utilities\n",
    "sys.path.insert(0, '../python')\n",
    "\n",
    "import debug\n",
    "\n",
    "from jbyrne_utils import load_data\n",
    "# from jbyrne_utils import tokenize_sentences\n",
    "# from jbyrne_utils import embed_matrix\n",
    "# from jbyrne_utils import run_model\n",
    "\n",
    "# Set message level\n",
    "\n",
    "# debug.off()\n",
    "# debug.on()\n",
    "debug.show_detail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "downtown-prescription",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters for the base model\n",
    "\n",
    "# maximum number of tokens to look at.\n",
    "max_len = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-browser",
   "metadata": {},
   "source": [
    "## Step 1:  Load the ClaimBuster datafile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "independent-entrance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11056 data records.\n"
     ]
    }
   ],
   "source": [
    "d = load_data(\"../data/3xNCS.json\")\n",
    "\n",
    "# Randomize the order as the data is sorted by class\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "intensive-wright",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence_id': 9703,\n",
       " 'label': 1,\n",
       " 'text': 'President Obama was right, he said that that was outrageous to have deficits as high as half a trillion dollars under the Bush years.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## View an random example entry\n",
    "d[512]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-sandwich",
   "metadata": {},
   "source": [
    "## Step 2:  Tokenize the sentences using BERT tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "concerned-express",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "tough-uniform",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************** DEBUG TURNED ON **********************\n",
      "11056 11056 11056 11056\n",
      "*************** DEBUG DETAILS TURNED ON *****************\n"
     ]
    }
   ],
   "source": [
    "input_sentences=[]\n",
    "input_ids=[]\n",
    "attention_masks=[]  # used so BERT can discount padding in the fixed-length token list\n",
    "\n",
    "# avoid big output for just this cell\n",
    "debug.on()\n",
    "\n",
    "for sentence in [ i[\"text\"] for i in d ]:\n",
    "    input_sentences.append(sentence)\n",
    "    bert_input = bert_tokenizer.encode_plus(sentence,\n",
    "                                            add_special_tokens=True,  # adds the CLS etc.\n",
    "                                            max_length=max_len,\n",
    "                                            truncation=True,          # truncate sentences over max_len\n",
    "                                            padding = 'max_length',   # add padding ids (0) up to max_len\n",
    "                                            return_attention_mask=True)\n",
    "    input_ids.append(bert_input['input_ids'])\n",
    "    attention_masks.append(bert_input['attention_mask'])\n",
    "    debug.detail(bert_input)\n",
    "\n",
    "    \n",
    "input_ids = np.asarray(input_ids)\n",
    "attention_masks = np.asarray(attention_masks)\n",
    "input_sentences = np.asarray(input_sentences)\n",
    "labels = np.array( [i[\"label\"] for i in d] )\n",
    "\n",
    "# check lengths of arrays\n",
    "debug.msg(len(input_ids), len(attention_masks), len(labels), len(input_sentences))\n",
    "\n",
    "# reset to previous debugging level\n",
    "debug.last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "still-large",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "President Obama was right, he said that that was outrageous to have deficits as high as half a trillion dollars under the Bush years.\n",
      "[  101  2343  8112  2001  2157  1010  2002  2056  2008  2008  2001 25506\n",
      "  2000  2031 15074  2015  2004  2152  2004  2431  1037 23458  6363  2104\n",
      "  1996  5747  2086  1012   102     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'president',\n",
       " 'obama',\n",
       " 'was',\n",
       " 'right',\n",
       " ',',\n",
       " 'he',\n",
       " 'said',\n",
       " 'that',\n",
       " 'that',\n",
       " 'was',\n",
       " 'outrageous',\n",
       " 'to',\n",
       " 'have',\n",
       " 'deficit',\n",
       " '##s',\n",
       " 'as',\n",
       " 'high',\n",
       " 'as',\n",
       " 'half',\n",
       " 'a',\n",
       " 'trillion',\n",
       " 'dollars',\n",
       " 'under',\n",
       " 'the',\n",
       " 'bush',\n",
       " 'years',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Verify the tokenization of the previous sample sentence\n",
    "\n",
    "debug.msg(d[512][\"text\"])\n",
    "debug.msg(input_ids[512])\n",
    "bert_tokenizer.convert_ids_to_tokens(input_ids[512])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-nightmare",
   "metadata": {},
   "source": [
    "## Step 3a: Split into training, validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "reported-convertible",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proportion of checkable claims in training data  : 0.2495\n",
      "proportion of checkable claims in validation data: 0.2518\n",
      "8844 8844 8844 8844\n",
      "2212 2212 2212 2212\n"
     ]
    }
   ],
   "source": [
    "train_len = int(0.8 * len(d))\n",
    "val_len = int(0.2 * len(d))\n",
    "\n",
    "train_ids, val_ids             = np.split(input_ids, [train_len])\n",
    "train_attn, val_attn           = np.split(attention_masks, [train_len])\n",
    "train_sentences, val_sentences = np.split(input_sentences, [train_len])\n",
    "train_labels, val_labels       = np.split(labels, [train_len])\n",
    "\n",
    "debug.msg(f\"proportion of checkable claims in training data  : {np.count_nonzero(train_labels == 1)/len(train_labels):.4f}\")\n",
    "debug.msg(f\"proportion of checkable claims in validation data: {np.count_nonzero(val_labels == 1)/len(val_labels):.4f}\")\n",
    "\n",
    "debug.detail(len(train_ids), len(train_attn), len(train_sentences), len(train_labels))\n",
    "debug.detail(len(val_ids), len(val_attn), len(val_sentences), len(val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pharmaceutical-ebony",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-black",
   "metadata": {},
   "source": [
    "## Step 3b: Even out the checkable and non-checkable classes.\n",
    "\n",
    "The intention here is to equalize the number of checkable and non-checkable sentences. In the raw dataset, approximately 25% of the statements are labelled as checkable claims.\n",
    "\n",
    "As we are looking at detailed text and whether it includes a checkable claim, there is no reliable equivalent of the data enhancement techniques that exist for image or sound data.  \n",
    "\n",
    "We are presented with the choice, therefore, of removing $\\frac{2}{3}$ of the non-checkable claims - as the source dataset has provided, or adding two copies of each checkable claim to reach approximately a 1:1 ratio of classes in the training data. The second method has proved especially successful in the CNN examples, so we will do the same for the BERT case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dental-circus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8844, 100)\n",
      "(8844, 100)\n",
      "(8844,)\n",
      "(8844,)\n"
     ]
    }
   ],
   "source": [
    "## Ideally we could rerandomize the training set, but\n",
    "## for the moment, we will try just adding copies of \n",
    "## the positive records to the end.\n",
    "\n",
    "pos_train_ids = train_ids[ train_labels == 1 ]\n",
    "pos_train_attn = train_attn[ train_labels == 1 ]\n",
    "pos_train_sentences = train_sentences[ train_labels == 1 ]\n",
    "pos_train_labels = train_labels[ train_labels == 1 ]  # kinda redundant, but an easy way to get the right length.\n",
    "\n",
    "print(train_ids.shape)\n",
    "print(train_attn.shape)\n",
    "print(train_sentences.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "\n",
    "## concatenate two copies of the positive cases to each of the training datasets\n",
    "\n",
    "train_ids       = np.concatenate( (train_ids, pos_train_ids, pos_train_ids) )\n",
    "train_attn      = np.concatenate( (train_attn, pos_train_attn, pos_train_attn) )\n",
    "train_sentences = np.concatenate( (train_sentences, pos_train_sentences, pos_train_sentences) )\n",
    "train_labels    = np.concatenate( (train_labels, pos_train_labels, pos_train_labels) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "closing-gathering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2207, 100)\n",
      "(2207, 100)\n",
      "(2207,)\n",
      "(2207,)\n",
      "(13258, 100)\n",
      "(13258, 100)\n",
      "(13258,)\n",
      "(13258,)\n"
     ]
    }
   ],
   "source": [
    "print(pos_train_ids.shape)\n",
    "print(pos_train_attn.shape)\n",
    "print(pos_train_sentences.shape)\n",
    "print(pos_train_labels.shape)\n",
    "\n",
    "print(train_ids.shape)\n",
    "print(train_attn.shape)\n",
    "print(train_sentences.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-minimum",
   "metadata": {},
   "source": [
    "## Step 4: Set up the Bert model\n",
    "\n",
    "Claim detection is a sentence classification task, so for the first runs, I will base this on the `TFBertForSequenceClassification` class from the huggungface Tensorflow implementation of Bert. \n",
    "\n",
    "To keep it simple, the model build is packaged into a build_bert_model function that returns a compiled model\n",
    "with the search space already added.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "formal-preview",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build the model with hyperparameter search spaces included\n",
    "\n",
    "def build_bert_model(hp):\n",
    "    \n",
    "    ################################################\n",
    "    ####  DEFINE THE HYPERPARAMETER SEARCH SPACE ###\n",
    "    ################################################\n",
    "    bert_trainable = hp.Choice('bert_trainable', values=[True, False])\n",
    "                               \n",
    "    optimizer      = keras.optimizers.Adam( hp.Choice('learning_rate',\n",
    "                                                      values=[5e-4, 2e-4, 1e-4, 5e-5, 2e-5, 1e-5, \\\n",
    "                                                              5e-6, 2e-6, 1e-6]),\n",
    "                                            hp.Choice('epsilon',\n",
    "                                                      values= [5e-8, 2e-8, 1e-8, 5e-7, 2e-7, 1e-7, \\\n",
    "                                                               5e-6, 2e-6, 1e-6, 5e-5, 2e-5, 1e-5]))\n",
    "    \n",
    "    loss           = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metric         = keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "    \n",
    "    ###############################\n",
    "    ####  CREATE THE BERT MODEL ###\n",
    "    ###############################\n",
    "    bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased',\n",
    "                                                                 num_labels=2,\n",
    "                                                                 trainable=bert_trainable)\n",
    "    print('\\nBert Model',bert_model.summary())\n",
    "\n",
    "    ############################################\n",
    "    ####  COMPILE THE MODEL WITH THE CHOICES ###\n",
    "    ############################################\n",
    "    bert_model.compile(loss=loss, optimizer=optimizer, metrics=[metric] )\n",
    "\n",
    "    return(bert_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-ready",
   "metadata": {},
   "source": [
    "## Step 5: Set up the Keras Tuner\n",
    "\n",
    "From https://github.com/keras-team/keras-tuner/README.md:\n",
    "\n",
    "***\n",
    "Next, instantiate a tuner. You should specify the model-building function, the name of the objective to optimize (whether to minimize or maximize is automatically inferred for built-in metrics), the total number of trials (max_trials) to test, and the number of models that should be built and fit for each trial (executions_per_trial).\n",
    "\n",
    "Available tuners are `RandomSearch` and `Hyperband`.\n",
    "\n",
    "Note: the purpose of having multiple executions per trial is to reduce results variance and therefore be able to more accurately assess the performance of a model. If you want to get results faster, you could set executions_per_trial=1 (single round of training for each model configuration).\n",
    "***\n",
    "\n",
    "More reading shows that there are additional tuners in `kt.tuners` like `kt.tuners.BayesianOptimization`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fixed-tribune",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 109,483,778\n",
      "Trainable params: 109,483,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Bert Model None\n"
     ]
    }
   ],
   "source": [
    "# Create the tuner object\n",
    "#\n",
    "# There are two options, RandomSearch and Hyperband.\n",
    "\n",
    "runtag = datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "log_dir='./tb_bert_tuner/'+ runtag\n",
    "\n",
    "tuner = kt.tuners.BayesianOptimization(\n",
    "    build_bert_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=5,\n",
    "    directory=log_dir,\n",
    "    project_name='BertTunerRandom')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-feeding",
   "metadata": {},
   "source": [
    "## Step 6: Create Callbacks\n",
    "\n",
    "Adding a new callback here, keras.callbacks.EarlyStopping(). This stops after the monitored metric (usually loss) stops improving.  The $patience$ term is the number of epochs that will be executed without improvement before stopping to allow for possible oscilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cardiovascular-schedule",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtag = datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "\n",
    "log_dir='./tb_bert/'+ runtag\n",
    "\n",
    "model_save_path='../models/bert_keras_tuner/' + runtag\n",
    "\n",
    "## Create Callback list\n",
    "callbacks = [keras.callbacks.ModelCheckpoint(filepath=model_save_path,\n",
    "                                             monitor='val_accuracy',\n",
    "                                             mode='max',\n",
    "                                             save_best_only=True)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-encyclopedia",
   "metadata": {},
   "source": [
    "## Step 7: Run the Optimizer\n",
    "\n",
    "Tuner.search() has the same signature (parameters) as keras.Model.fit()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-bridge",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [02h 04m 16s]\n",
      "val_accuracy: 0.9304701566696167\n",
      "\n",
      "Best val_accuracy So Far: 0.9304701566696167\n",
      "Total elapsed time: 02h 04m 16s\n",
      "\n",
      "Search: Running Trial #2\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "bert_trainable    |1                 |1                 \n",
      "learning_rate     |0.0001            |2e-05             \n",
      "epsilon           |5e-05             |1e-08             \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 109,483,778\n",
      "Trainable params: 109,483,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Bert Model None\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "553/553 [==============================] - ETA: 0s - loss: 0.3819 - accuracy: 0.8313WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "553/553 [==============================] - 156s 268ms/step - loss: 0.3817 - accuracy: 0.8314 - val_loss: 0.2653 - val_accuracy: 0.8987\n",
      "Epoch 2/10\n",
      "553/553 [==============================] - 147s 265ms/step - loss: 0.1059 - accuracy: 0.9649 - val_loss: 0.3532 - val_accuracy: 0.8938\n",
      "Epoch 3/10\n",
      "553/553 [==============================] - 147s 266ms/step - loss: 0.0585 - accuracy: 0.9813 - val_loss: 0.3502 - val_accuracy: 0.9096\n",
      "Epoch 4/10\n",
      "553/553 [==============================] - 147s 265ms/step - loss: 0.0476 - accuracy: 0.9857 - val_loss: 0.3049 - val_accuracy: 0.9087\n",
      "Epoch 5/10\n",
      "553/553 [==============================] - 147s 265ms/step - loss: 0.0342 - accuracy: 0.9895 - val_loss: 0.4030 - val_accuracy: 0.9005\n",
      "Epoch 6/10\n",
      "553/553 [==============================] - 147s 265ms/step - loss: 0.0384 - accuracy: 0.9885 - val_loss: 0.4568 - val_accuracy: 0.8960\n",
      "Epoch 7/10\n",
      "553/553 [==============================] - 147s 266ms/step - loss: 0.0471 - accuracy: 0.9866 - val_loss: 0.5503 - val_accuracy: 0.8951\n",
      "Epoch 8/10\n",
      "553/553 [==============================] - 146s 264ms/step - loss: 0.0253 - accuracy: 0.9920 - val_loss: 0.6348 - val_accuracy: 0.8938\n",
      "Epoch 9/10\n",
      "553/553 [==============================] - 146s 264ms/step - loss: 0.1521 - accuracy: 0.9110 - val_loss: 0.7048 - val_accuracy: 0.2518\n",
      "Epoch 10/10\n",
      "553/553 [==============================] - 146s 265ms/step - loss: 0.7012 - accuracy: 0.5038 - val_loss: 0.6920 - val_accuracy: 0.7482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 109,483,778\n",
      "Trainable params: 109,483,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Bert Model None\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "553/553 [==============================] - ETA: 0s - loss: 0.3783 - accuracy: 0.8308WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "553/553 [==============================] - 158s 272ms/step - loss: 0.3781 - accuracy: 0.8309 - val_loss: 0.2609 - val_accuracy: 0.9127\n",
      "Epoch 2/10\n",
      "553/553 [==============================] - 147s 266ms/step - loss: 0.1318 - accuracy: 0.9589 - val_loss: 0.3524 - val_accuracy: 0.9051\n",
      "Epoch 3/10\n",
      "553/553 [==============================] - 148s 268ms/step - loss: 0.0665 - accuracy: 0.9781 - val_loss: 0.3935 - val_accuracy: 0.9037\n",
      "Epoch 4/10\n",
      "553/553 [==============================] - 148s 267ms/step - loss: 0.0454 - accuracy: 0.9868 - val_loss: 0.4753 - val_accuracy: 0.8766\n",
      "Epoch 5/10\n",
      "553/553 [==============================] - 148s 267ms/step - loss: 0.0370 - accuracy: 0.9885 - val_loss: 0.4109 - val_accuracy: 0.9064\n",
      "Epoch 6/10\n",
      "553/553 [==============================] - 147s 266ms/step - loss: 0.0407 - accuracy: 0.9890 - val_loss: 0.4845 - val_accuracy: 0.9001\n",
      "Epoch 7/10\n",
      "553/553 [==============================] - 147s 266ms/step - loss: 0.0363 - accuracy: 0.9853 - val_loss: 0.6085 - val_accuracy: 0.7482\n",
      "Epoch 8/10\n",
      "553/553 [==============================] - 147s 266ms/step - loss: 0.7024 - accuracy: 0.5132 - val_loss: 0.7795 - val_accuracy: 0.2518\n",
      "Epoch 9/10\n",
      "553/553 [==============================] - 146s 265ms/step - loss: 0.7024 - accuracy: 0.5015 - val_loss: 0.6143 - val_accuracy: 0.7482\n",
      "Epoch 10/10\n",
      "553/553 [==============================] - 146s 264ms/step - loss: 0.7021 - accuracy: 0.4936 - val_loss: 0.6265 - val_accuracy: 0.7482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 109,483,778\n",
      "Trainable params: 109,483,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Bert Model None\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "553/553 [==============================] - ETA: 0s - loss: 0.3749 - accuracy: 0.8298WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "553/553 [==============================] - 156s 268ms/step - loss: 0.3747 - accuracy: 0.8299 - val_loss: 0.2833 - val_accuracy: 0.8969\n",
      "Epoch 2/10\n",
      "553/553 [==============================] - 147s 265ms/step - loss: 0.1126 - accuracy: 0.9625 - val_loss: 0.3658 - val_accuracy: 0.8856\n",
      "Epoch 3/10\n",
      "553/553 [==============================] - 146s 263ms/step - loss: 0.0667 - accuracy: 0.9770 - val_loss: 0.3522 - val_accuracy: 0.9100\n",
      "Epoch 4/10\n",
      "553/553 [==============================] - 146s 264ms/step - loss: 0.0292 - accuracy: 0.9906 - val_loss: 0.4381 - val_accuracy: 0.9042\n",
      "Epoch 5/10\n",
      "553/553 [==============================] - 145s 263ms/step - loss: 0.0547 - accuracy: 0.9818 - val_loss: 0.4566 - val_accuracy: 0.8929\n",
      "Epoch 6/10\n",
      "553/553 [==============================] - 146s 264ms/step - loss: 0.0277 - accuracy: 0.9909 - val_loss: 0.4547 - val_accuracy: 0.8956\n",
      "Epoch 7/10\n",
      "553/553 [==============================] - 146s 264ms/step - loss: 0.0237 - accuracy: 0.9930 - val_loss: 0.4142 - val_accuracy: 0.8820\n",
      "Epoch 8/10\n",
      "553/553 [==============================] - 148s 268ms/step - loss: 0.0537 - accuracy: 0.9839 - val_loss: 0.4120 - val_accuracy: 0.8915\n",
      "Epoch 9/10\n",
      "553/553 [==============================] - 147s 265ms/step - loss: 0.0206 - accuracy: 0.9941 - val_loss: 0.5806 - val_accuracy: 0.8938\n",
      "Epoch 10/10\n",
      "553/553 [==============================] - 148s 267ms/step - loss: 0.0455 - accuracy: 0.9770 - val_loss: 0.7044 - val_accuracy: 0.2518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 109,483,778\n",
      "Trainable params: 109,483,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Bert Model None\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "553/553 [==============================] - ETA: 0s - loss: 0.3965 - accuracy: 0.8236WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "553/553 [==============================] - 157s 270ms/step - loss: 0.3963 - accuracy: 0.8237 - val_loss: 0.3645 - val_accuracy: 0.8671\n",
      "Epoch 2/10\n",
      "553/553 [==============================] - 147s 265ms/step - loss: 0.1107 - accuracy: 0.9638 - val_loss: 0.2918 - val_accuracy: 0.9168\n",
      "Epoch 3/10\n",
      "553/553 [==============================] - 147s 266ms/step - loss: 0.0708 - accuracy: 0.9772 - val_loss: 0.4626 - val_accuracy: 0.9005\n",
      "Epoch 4/10\n",
      "553/553 [==============================] - 148s 267ms/step - loss: 0.0416 - accuracy: 0.9870 - val_loss: 0.3653 - val_accuracy: 0.9127\n",
      "Epoch 5/10\n",
      "125/553 [=====>........................] - ETA: 1:47 - loss: 0.0121 - accuracy: 0.9955"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tuner.search([train_ids,train_attn],\n",
    "             train_labels,\n",
    "             batch_size=24,\n",
    "             epochs=10,\n",
    "             validation_data=([val_ids,val_attn],val_labels),\n",
    "             callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-template",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-banana",
   "metadata": {},
   "source": [
    "## References used\n",
    "BERT Text Classification using Keras https://swatimeena989.medium.com/bert-text-classification-using-keras-903671e0207d#2f06\n",
    "Keras Tuner blog: https://blog.tensorflow.org/2020/01/hyperparameter-tuning-with-keras-tuner.html\n",
    "Keras Tuner Git:  https://github.com/keras-team/keras-tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-logistics",
   "metadata": {},
   "source": [
    "@misc{omalley2019kerastuner,\n",
    "\ttitle        = {Keras {Tuner}},\n",
    "\tauthor       = {O'Malley, Tom and Bursztein, Elie and Long, James and Chollet, Fran\\c{c}ois and Jin, Haifeng and Invernizzi, Luca and others},\n",
    "\tyear         = 2019,\n",
    "\thowpublished = {\\url{https://github.com/keras-team/keras-tuner}}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
