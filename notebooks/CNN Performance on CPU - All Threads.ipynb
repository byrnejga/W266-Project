{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "declared-amino",
   "metadata": {},
   "source": [
    "## Comparison Approach\n",
    "This notebook loads each of the individual, trained models from the best runs of both Bert and CNN-based approaches. It will show the model.summary() and diagram, then will run a performance test by inferring results for the texts in the ClaimBuster dataset's crowdsourced.csv file. The file contains 22501 sentences. We will use sentences per second as the performance metric, and the on-disk size of each model as the complexity metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rising-color",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Usual Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as backend\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.config.experimental import list_physical_devices, set_visible_devices\n",
    "\n",
    "import string\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import matplotlib as plt\n",
    "\n",
    "import datetime\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../python')\n",
    "import debug\n",
    "from jbyrne_utils import tokenize_sentences\n",
    "\n",
    "\n",
    "# to fix the CUDA issues for CUDA 11.2 to allow use of the GPU\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-mailing",
   "metadata": {},
   "source": [
    "### Load the crowdsourced test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "absent-decision",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and parse the crowdsourced.csv file\n",
    "\n",
    "cs = pd.read_csv(\"../data/crowdsourced.csv\", delimiter=',', quotechar = '\"', index_col='Sentence_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-quick",
   "metadata": {},
   "source": [
    "Unlike the curated json dataset we used for training, the \"verdict\" column takes three values:\n",
    "\n",
    "| Verdict | Description |\n",
    "| :---: | :--- |\n",
    "| +1 | Checkable Fact Statements, e.g. \"Inflation is down 2%\" |\n",
    "| 0 | Uncheckable Fact Statements, e.g. \"Jack likes fish\" |\n",
    "| -1 | Non Fact Statements, e.g. \"Drink the water\" |\n",
    "\n",
    "For the purposes of this paper, we are only interested in checkable fact statements, so we set any -1 verdicts to equal zero before tokenizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sharing-animal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14685"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cs.loc[cs[\"Verdict\"] == -1][\"Verdict\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "elect-weather",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 14685 -1 labels.\n",
      "After:  0 -1 labels.\n"
     ]
    }
   ],
   "source": [
    "# Change -1 verdicts (non claim sentences) to be 0.\n",
    "print(f\"Before: {len(cs.loc[cs['Verdict'] == -1])} -1 labels.\")\n",
    "\n",
    "cs.loc[cs[\"Verdict\"] == -1, \"Verdict\"] = 0\n",
    "\n",
    "print(f\"After:  {len(cs.loc[cs['Verdict'] == -1])} -1 labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-webmaster",
   "metadata": {},
   "source": [
    "### Tokenizing the new dataset\n",
    "Provided this is run AFTER the other tests, there should be a tokenizer.pkl and embed_matrix.pkl already created from the training dataset.  We need to encode the new text using the same vocabulary and ID mapping as it will be input into a pre-trained embeddings layer in the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-brazil",
   "metadata": {},
   "source": [
    "## Initialize the output dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abandoned-fashion",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(columns = [\"Type\",\n",
    "                                 \"Model\",\n",
    "                                 \"Hardware\",\n",
    "                                 \"Max Length\",\n",
    "                                 \"Filters\",\n",
    "                                 \"Dense Layers\",\n",
    "                                 \"Parameter Count\",\n",
    "                                 \"Val Accuracy\",\n",
    "                                 \"Test Accuracy\",\n",
    "                                 \"Inf. Rate/s\",\n",
    "                                 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-ecuador",
   "metadata": {},
   "source": [
    "### GPU vs CPU performance\n",
    "As one objective is to run claim detection at the edge, we will be doing performance testing on both GPU and CPU hardware.\n",
    "\n",
    "All work on this project has been done using the following software and hardware:\n",
    "\n",
    "* Anaconda distribution of Python 3.8.2\n",
    "* Tensorflow 2.4.1\n",
    "* AMD Ryzen TR 3970X 32-Core Processor with Hyperthreading (64 threads)\n",
    "* NVidia RTX2080 Super GPU\n",
    "\n",
    "First display the tensorflow IDs for the CPU and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fluid-rolling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "facial-trace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable the GPU for these runs\n",
    "set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-airplane",
   "metadata": {},
   "source": [
    "## Test the CNN Best models for each max_len\n",
    "We ran a series of grid searches to find the best performing models for each of the tested sentence lengths:\n",
    "\n",
    "| max_len | Description |\n",
    "| :---: | :--- |\n",
    "| 100 | Only 14 of 11056 sentences are truncated, so 99.87% of sentences are processed in full |\n",
    "| 50 | Half the length still processes 95.57% of sentences in full |\n",
    "| 21 | Equal to the rounded average length, processes 61.478% of sentences in full |\n",
    "| 17 | Equal to the median length, processes half the sentences in full |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acting-shell",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>max_len</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>embed_dim</th>\n",
       "      <th>num_filters</th>\n",
       "      <th>kernel_sizes</th>\n",
       "      <th>dense_layer_dims</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>val_accuracy_best</th>\n",
       "      <th>val_accuracy_best_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>210409-235328</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>[96, 96, 96]</td>\n",
       "      <td>[8, 16, 32]</td>\n",
       "      <td>[8]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.976492</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>210409-210515</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>[4, 8, 16]</td>\n",
       "      <td>[8]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.975588</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>210409-225707</td>\n",
       "      <td>17</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>[96, 96, 96]</td>\n",
       "      <td>[8, 12, 16]</td>\n",
       "      <td>[32]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.970615</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>210410-004542</td>\n",
       "      <td>21</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>[8, 12, 16]</td>\n",
       "      <td>[8]</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.969259</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       timestamp max_len batch_size embed_dim   num_filters kernel_sizes  \\\n",
       "1  210409-235328      50         50        50  [96, 96, 96]  [8, 16, 32]   \n",
       "0  210409-210515     100         50        50  [64, 64, 64]   [4, 8, 16]   \n",
       "3  210409-225707      17         50        50  [96, 96, 96]  [8, 12, 16]   \n",
       "2  210410-004542      21         50        50  [64, 64, 64]  [8, 12, 16]   \n",
       "\n",
       "  dense_layer_dims  dropout_rate  val_accuracy_best val_accuracy_best_epoch  \n",
       "1              [8]           0.2           0.976492                       6  \n",
       "0              [8]           0.2           0.975588                       8  \n",
       "3             [32]           0.2           0.970615                      19  \n",
       "2              [8]           0.2           0.969259                      14  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./best_models.pkl', 'rb') as f:\n",
    "    best_models = pickle.load(f)\n",
    "best_models = best_models.sort_values('val_accuracy_best', ascending=False)\n",
    "best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "numerical-highlight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    210409-235328\n",
       "0    210409-210515\n",
       "3    210409-225707\n",
       "2    210410-004542\n",
       "Name: timestamp, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_models[\"timestamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "consecutive-apple",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "CNN Model from timestamp 210409-235328\n",
      "Model: \"model_84\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_85 (InputLayer)           [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_84 (Embedding)        (None, 50, 50)       409800      input_85[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_228 (Conv1D)             (None, 43, 96)       38496       embedding_84[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_229 (Conv1D)             (None, 35, 96)       76896       embedding_84[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_230 (Conv1D)             (None, 19, 96)       153696      embedding_84[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_228 (Globa (None, 96)           0           conv1d_228[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_229 (Globa (None, 96)           0           conv1d_229[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_230 (Globa (None, 96)           0           conv1d_230[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_84 (Concatenate)    (None, 288)          0           global_max_pooling1d_228[0][0]   \n",
      "                                                                 global_max_pooling1d_229[0][0]   \n",
      "                                                                 global_max_pooling1d_230[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_84 (Dropout)            (None, 288)          0           concatenate_84[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_189 (Dense)               (None, 8)            2312        dropout_84[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_190 (Dense)               (None, 1)            9           dense_189[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 681,209\n",
      "Trainable params: 681,209\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Loading previously created Tokenizer\n",
      "Inferring 100 iterations of test data\n",
      "\n",
      "000.........\n",
      "010.........\n",
      "020.........\n",
      "030.........\n",
      "040.........\n",
      "050.........\n",
      "060.........\n",
      "070.........\n",
      "080.........\n",
      "090.........\n",
      "\n",
      "COMPLETED\n",
      "\n",
      "Time taken for 2,250,100 inferrences = 54.488 s.\n",
      "Rate is 41,294.983 inferrences per second\n",
      "\n",
      "\n",
      "\n",
      "CNN Model from timestamp 210409-210515\n",
      "Model: \"model_60\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_61 (InputLayer)           [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_60 (Embedding)        (None, 100, 50)      409800      input_61[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_144 (Conv1D)             (None, 97, 64)       12864       embedding_60[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_145 (Conv1D)             (None, 93, 64)       25664       embedding_60[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_146 (Conv1D)             (None, 85, 64)       51264       embedding_60[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_144 (Globa (None, 64)           0           conv1d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_145 (Globa (None, 64)           0           conv1d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_146 (Globa (None, 64)           0           conv1d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_60 (Concatenate)    (None, 192)          0           global_max_pooling1d_144[0][0]   \n",
      "                                                                 global_max_pooling1d_145[0][0]   \n",
      "                                                                 global_max_pooling1d_146[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_60 (Dropout)            (None, 192)          0           concatenate_60[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_135 (Dense)               (None, 8)            1544        dropout_60[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_136 (Dense)               (None, 1)            9           dense_135[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 501,145\n",
      "Trainable params: 501,145\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Loading previously created Tokenizer\n",
      "Inferring 100 iterations of test data\n",
      "\n",
      "000.........\n",
      "010.........\n",
      "020.........\n",
      "030.........\n",
      "040.........\n",
      "050.........\n",
      "060.........\n",
      "070.........\n",
      "080.........\n",
      "090.........\n",
      "\n",
      "COMPLETED\n",
      "\n",
      "Time taken for 2,250,100 inferrences = 50.896 s.\n",
      "Rate is 44,209.783 inferrences per second\n",
      "\n",
      "\n",
      "\n",
      "CNN Model from timestamp 210409-225707\n",
      "Model: \"model_58\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_59 (InputLayer)           [(None, 17)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_58 (Embedding)        (None, 17, 50)       409800      input_59[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_162 (Conv1D)             (None, 10, 96)       38496       embedding_58[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_163 (Conv1D)             (None, 6, 96)        57696       embedding_58[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_164 (Conv1D)             (None, 2, 96)        76896       embedding_58[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_162 (Globa (None, 96)           0           conv1d_162[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_163 (Globa (None, 96)           0           conv1d_163[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_164 (Globa (None, 96)           0           conv1d_164[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_58 (Concatenate)    (None, 288)          0           global_max_pooling1d_162[0][0]   \n",
      "                                                                 global_max_pooling1d_163[0][0]   \n",
      "                                                                 global_max_pooling1d_164[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_58 (Dropout)            (None, 288)          0           concatenate_58[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_130 (Dense)               (None, 32)           9248        dropout_58[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_131 (Dense)               (None, 1)            33          dense_130[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 592,169\n",
      "Trainable params: 592,169\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previously created Tokenizer\n",
      "Inferring 100 iterations of test data\n",
      "\n",
      "000.........\n",
      "010.........\n",
      "020.........\n",
      "030.........\n",
      "040.........\n",
      "050.........\n",
      "060.........\n",
      "070.........\n",
      "080.........\n",
      "090.........\n",
      "\n",
      "COMPLETED\n",
      "\n",
      "Time taken for 2,250,100 inferrences = 24.028 s.\n",
      "Rate is 93,644.918 inferrences per second\n",
      "\n",
      "\n",
      "\n",
      "CNN Model from timestamp 210410-004542\n",
      "Model: \"model_52\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_53 (InputLayer)           [(None, 21)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_52 (Embedding)        (None, 21, 50)       409800      input_53[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_144 (Conv1D)             (None, 14, 64)       25664       embedding_52[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_145 (Conv1D)             (None, 10, 64)       38464       embedding_52[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_146 (Conv1D)             (None, 6, 64)        51264       embedding_52[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_144 (Globa (None, 64)           0           conv1d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_145 (Globa (None, 64)           0           conv1d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_146 (Globa (None, 64)           0           conv1d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_52 (Concatenate)    (None, 192)          0           global_max_pooling1d_144[0][0]   \n",
      "                                                                 global_max_pooling1d_145[0][0]   \n",
      "                                                                 global_max_pooling1d_146[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_52 (Dropout)            (None, 192)          0           concatenate_52[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_117 (Dense)               (None, 8)            1544        dropout_52[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_118 (Dense)               (None, 1)            9           dense_117[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 526,745\n",
      "Trainable params: 526,745\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Loading previously created Tokenizer\n",
      "Inferring 100 iterations of test data\n",
      "\n",
      "000.........\n",
      "010.........\n",
      "020.........\n",
      "030.........\n",
      "040.........\n",
      "050.........\n",
      "060.........\n",
      "070.........\n",
      "080.........\n",
      "090.........\n",
      "\n",
      "COMPLETED\n",
      "\n",
      "Time taken for 2,250,100 inferrences = 26.712 s.\n",
      "Rate is 84,235.316 inferrences per second\n"
     ]
    }
   ],
   "source": [
    "cycles = 100\n",
    "\n",
    "\n",
    "def run_perftest(tokens, labels, model, cycles):\n",
    "    print(f\"Inferring {cycles} iterations of test data\")\n",
    "    start_time = datetime.datetime.now()\n",
    "    for i in range(cycles):\n",
    "        history = model.evaluate(tokens, labels, batch_size=128, verbose=0)\n",
    "        if i % 10 == 0:\n",
    "            print(f\"\\n{i:03d}\", end=\"\")\n",
    "        else:\n",
    "            print(\".\", end=\"\")\n",
    "    print('\\n\\nCOMPLETED\\n')\n",
    "                  \n",
    "    end_time = datetime.datetime.now()\n",
    "    difference = end_time - start_time\n",
    "    return (difference.total_seconds(), history)\n",
    "\n",
    "keras.backend.clear_session()\n",
    "for index,row in best_models.iterrows():\n",
    "    timestamp = row[\"timestamp\"]\n",
    "    \n",
    "    model = keras.models.load_model(f'../best_models/{timestamp}')\n",
    "    print(f\"\\n\\n\\nCNN Model from timestamp {timestamp}\")\n",
    "    \n",
    "    # Display summary and diagram of the model\n",
    "    model.summary()\n",
    "    keras.utils.plot_model(model, f'{timestamp}.png', show_shapes=True, show_dtype=True, rankdir=\"TB\")\n",
    "    \n",
    "    tokens, _ = tokenize_sentences(cs[\"Text\"], max_len=row[\"max_len\"] )\n",
    "    labels = cs[\"Verdict\"]\n",
    "    \n",
    "    # Run Performance Test on the crowdsourced test data set\n",
    "    \n",
    "    difference, history = run_perftest(tokens, labels, model, cycles)\n",
    "    \n",
    "    print(f\"Time taken for {cycles * len(labels):,} inferrences = {difference:.3f} s.\")\n",
    "    print(f\"Rate is {cycles * len(labels) / difference:,.3f} inferrences per second\")\n",
    "    \n",
    "    # Add the results to the output\n",
    "    \n",
    "    filter_string = f\"Sizes: {row['kernel_sizes']} Counts: {row['num_filters']}\"\n",
    "    parameter_count = np.sum([backend.count_params(w) for w in model.trainable_weights]) + \\\n",
    "                      np.sum([backend.count_params(w) for w in model.non_trainable_weights])\n",
    "\n",
    "    \n",
    "    record = pd.DataFrame( {\"Type\": \"CNN\",\n",
    "                            \"Model\": timestamp,\n",
    "                            \"Hardware\": \"GPU\",\n",
    "                            \"Max Length\": row[\"max_len\"],\n",
    "                            \"Filters\": filter_string,\n",
    "                            \"Dense Layers\": row[\"dense_layer_dims\"],\n",
    "                            \"Parameter Count\": parameter_count ,\n",
    "                            \"Val Accuracy\": row[\"val_accuracy_best\"],\n",
    "                            \"Test Accuracy\": history[1],\n",
    "                            \"Inf. Rate/s\": cycles * len(labels) / difference\n",
    "                           },\n",
    "                           index = [1]) # timestamp\n",
    "    output = output.append(record)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "mechanical-patrol",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Model</th>\n",
       "      <th>Hardware</th>\n",
       "      <th>Max Length</th>\n",
       "      <th>Filters</th>\n",
       "      <th>Dense Layers</th>\n",
       "      <th>Parameter Count</th>\n",
       "      <th>Val Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Inf. Rate/s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNN</td>\n",
       "      <td>210409-235328</td>\n",
       "      <td>GPU</td>\n",
       "      <td>50</td>\n",
       "      <td>Sizes: [8, 16, 32] Counts: [96, 96, 96]</td>\n",
       "      <td>[8]</td>\n",
       "      <td>681209.0</td>\n",
       "      <td>0.976492</td>\n",
       "      <td>0.886361</td>\n",
       "      <td>41294.983218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNN</td>\n",
       "      <td>210409-210515</td>\n",
       "      <td>GPU</td>\n",
       "      <td>100</td>\n",
       "      <td>Sizes: [4, 8, 16] Counts: [64, 64, 64]</td>\n",
       "      <td>[8]</td>\n",
       "      <td>501145.0</td>\n",
       "      <td>0.975588</td>\n",
       "      <td>0.889072</td>\n",
       "      <td>44209.782797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNN</td>\n",
       "      <td>210409-225707</td>\n",
       "      <td>GPU</td>\n",
       "      <td>17</td>\n",
       "      <td>Sizes: [8, 12, 16] Counts: [96, 96, 96]</td>\n",
       "      <td>[32]</td>\n",
       "      <td>592169.0</td>\n",
       "      <td>0.970615</td>\n",
       "      <td>0.885072</td>\n",
       "      <td>93644.918164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNN</td>\n",
       "      <td>210410-004542</td>\n",
       "      <td>GPU</td>\n",
       "      <td>21</td>\n",
       "      <td>Sizes: [8, 12, 16] Counts: [64, 64, 64]</td>\n",
       "      <td>[8]</td>\n",
       "      <td>526745.0</td>\n",
       "      <td>0.969259</td>\n",
       "      <td>0.886494</td>\n",
       "      <td>84235.316209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Type          Model Hardware Max Length  \\\n",
       "1  CNN  210409-235328      GPU         50   \n",
       "1  CNN  210409-210515      GPU        100   \n",
       "1  CNN  210409-225707      GPU         17   \n",
       "1  CNN  210410-004542      GPU         21   \n",
       "\n",
       "                                   Filters Dense Layers  Parameter Count  \\\n",
       "1  Sizes: [8, 16, 32] Counts: [96, 96, 96]          [8]         681209.0   \n",
       "1   Sizes: [4, 8, 16] Counts: [64, 64, 64]          [8]         501145.0   \n",
       "1  Sizes: [8, 12, 16] Counts: [96, 96, 96]         [32]         592169.0   \n",
       "1  Sizes: [8, 12, 16] Counts: [64, 64, 64]          [8]         526745.0   \n",
       "\n",
       "   Val Accuracy  Test Accuracy   Inf. Rate/s  \n",
       "1      0.976492       0.886361  41294.983218  \n",
       "1      0.975588       0.889072  44209.782797  \n",
       "1      0.970615       0.885072  93644.918164  \n",
       "1      0.969259       0.886494  84235.316209  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-biography",
   "metadata": {},
   "source": [
    "__210409-235328: max_len = 50__\n",
    "![\"210409-235328\"](./210409-235328.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-essex",
   "metadata": {},
   "source": [
    "__210409-210515\tmax_len=100__\n",
    "![\"210409-210515\"](./210409-210515.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-tulsa",
   "metadata": {},
   "source": [
    "__210409-225707\tmax_len=17__\n",
    "![\"210409-225707\"](./210409-225707.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-remove",
   "metadata": {},
   "source": [
    "__210410-004542\tmax_len=21__\n",
    "![\"210410-004542\"](./210410-004542.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-fiber",
   "metadata": {},
   "source": [
    "### Citations\n",
    "@inproceedings{arslan2020claimbuster,\n",
    "    title={{A Benchmark Dataset of Check-worthy Factual Claims}},\n",
    "    author={Arslan, Fatma and Hassan, Naeemul and Li, Chengkai and Tremayne, Mark },\n",
    "    booktitle={14th International AAAI Conference on Web and Social Media},\n",
    "    year={2020},\n",
    "    organization={AAAI}\n",
    "}\n",
    "\n",
    "@article{meng2020gradient,\n",
    "  title={Gradient-Based Adversarial Training on Transformer Networks for Detecting Check-Worthy Factual Claims},\n",
    "  author={Meng, Kevin and Jimenez, Damian and Arslan, Fatma and Devasier, Jacob Daniel and Obembe, Daniel and Li, Chengkai},\n",
    "  journal={arXiv preprint arXiv:2002.07725},\n",
    "  year={2020}\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
